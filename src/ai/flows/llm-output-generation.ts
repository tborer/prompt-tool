// src/ai/flows/llm-output-generation.ts
'use server';

/**
 * @fileOverview Flow to generate output from the LLM based on selected prompt and user inputs.
 *
 * - generateLlmOutput - A function that handles the generation of output from the LLM.
 * - GenerateLlmOutputInput - The input type for the generateLlmOutput function.
 * - GenerateLlmOutputOutput - The return type for the generateLlmOutput function.
 */

import {ai} from '@/lib/genkit';
import {z} from 'genkit';

const GenerateLlmOutputInputSchema = z.object({
  promptType: z.enum(['Prompt 1', 'Prompt 2']).describe('The type of prompt to use.'),
  field1: z.string().optional().describe('Field 1 input.'),
  field2: z.string().optional().describe('Field 2 input.'),
  field3: z.string().optional().describe('Field 3 input.'),
  field4: z.string().optional().describe('Field 4 input.'),
  field5: z.string().optional().describe('Field 5 input.'),
  field6: z.string().optional().describe('Field 6 input.'),
  prompt1Setup: z.string().describe('Prompt 1 setup text.'),
  prompt2Setup: z.string().describe('Prompt 2 setup text.'),
  llmApiKey: z.string().describe('The LLM API key.'),
});

export type GenerateLlmOutputInput = z.infer<typeof GenerateLlmOutputInputSchema>;

const GenerateLlmOutputOutputSchema = z.object({
  output: z.string().describe('The output generated by the LLM.'),
});

export type GenerateLlmOutputOutput = z.infer<typeof GenerateLlmOutputOutputSchema>;

export async function generateLlmOutput(input: GenerateLlmOutputInput): Promise<GenerateLlmOutputOutput> {
  console.log('generateLlmOutput called with input:', input);
  return generateLlmOutputFlow(input);
}

const prompt = ai.definePrompt({
  name: 'llmOutputPrompt',
  input: {schema: z.object({ finalPrompt: z.string() })},
  output: {schema: GenerateLlmOutputOutputSchema},
  config: { // Updated config for Gemini API message format
    model: 'gemini-2.0-flash' // Specify the model name
  }
});

const generateLlmOutputFlow = ai.defineFlow(
  {
    name: 'generateLlmOutputFlow',
    inputSchema: GenerateLlmOutputInputSchema,
    outputSchema: GenerateLlmOutputOutputSchema,
  },
  async input => {
    console.log('generateLlmOutputFlow entered with input:', input);
    if (input.promptType === 'Prompt 1' && !input.prompt1Setup) {
      throw new Error('Prompt 1 Setup is required.');
    }

    if (input.promptType === 'Prompt 2' && !input.prompt2Setup) {
      throw new Error('Prompt 2 Setup is required.');
    }

    if (input.promptType === 'Prompt 1' && (!input.field1 || !input.field2 || !input.field3)) {
      // Don't throw error here, because the fields are optional
    }

     if (input.promptType === 'Prompt 2' && (!input.field4 || !input.field5 || !input.field6)) {
      // Don't throw error here, because the fields are optional
    }

    const {
      field1,
      field2,
      field3,
      field4,
      field5,
      field6,
      prompt1Setup,
      prompt2Setup,
      promptType
    } = input

    try {
      console.log('Starting variable replacement...');
      const processedPrompt1Setup = replaceVariables(prompt1Setup, {field1, field2, field3});
      const processedPrompt2Setup = replaceVariables(prompt2Setup, {field4, field5, field6});
      console.log('Variable replacement finished.');

      const finalPrompt = promptType === 'Prompt 1' ? processedPrompt1Setup : processedPrompt2Setup;

      console.log('Actual prompt sent to LLM:', finalPrompt);

      console.log('Calling LLM API...');

      const { output } = await prompt({
        finalPrompt,
      });
      console.log('Raw LLM response:', output);

      let finalOutputString = '';
      if (typeof output === 'string') {
        finalOutputString = output;
      } else if (typeof output === 'object' && output !== null && 'output' in output && typeof output.output === 'string') {
        finalOutputString = output.output;
      } else {
        console.error('Unexpected LLM response structure:', output);
        finalOutputString = 'Error: Unexpected response from LLM.'; // Default error message
      }
      console.log('generateLlmOutputFlow exiting successfully with output:', finalOutputString);
      return { output: finalOutputString };
    } catch (error) {
      console.error('Error generating LLM output:', error);
 throw error; // Re-throw the error after logging
      console.log('generateLlmOutputFlow exiting with error.');
    }
  }
);

function replaceVariables(template: string, data: Record<string, string | undefined>): string {
  let result = template;
  for (const key in data) {
    if (data.hasOwnProperty(key)) {
      const value = data[key] || ''; // Use an empty string if the value is undefined
      const regex = new RegExp(`\{${key}\}`, 'g');
      result = result.replace(regex, value);
    }
  }
  return result;
}
